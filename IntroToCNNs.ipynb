{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b703a7b3",
   "metadata": {},
   "source": [
    "## Introduction to Convolutional Neural Networks\n",
    "\n",
    "- Basic concepts behind `Convolutional Neural Networks`;  a type of artificial neural network used in image recognition & processing that is specifically designed to **`- PROCESS PIXEL DATA -`**\n",
    "\n",
    "\n",
    "- **Part One**: Learn to implement a Convolutional Neural Network classifier with `TensorFlow 2.0` & `Keras`\n",
    "\n",
    "\n",
    "- **Part Two**: Evaluate the CNN after it was trained\n",
    "\n",
    "\n",
    "#### Rerference\n",
    "\n",
    "- **Conv2D** layers provide magnifier like operations at two-dimensions, ie, it slides a small 2D window over a larger 2D window - (the window being the image itself)\n",
    "\n",
    "\n",
    "- **Dense** layers are used later on for generating predictions, ie classifications\n",
    "\n",
    "\n",
    "- **Max pooling** layers are (typically) added after a Conv2D layer to provide an additional magnifier operation. It also slides a window over the image, & selects the maximum value for further propagation\n",
    "\n",
    "\n",
    "- **Flatten** connects the convolutional parts of the layer with the dense parts. Dense layers can only handle flat data, ie one-dimensional data, however convolutional layers are anything but 1D. Flatten takes all dimensions & concatenates them after each other\n",
    "\n",
    "\n",
    "- **Dropout** introduces a small amount of random noise into the summary during Training. Essentially, it breaks a bit of the magnifier in an effort to improve model performance & reduce overfitting. This way, unsual images can potentially be classified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276da6bd",
   "metadata": {},
   "source": [
    "### 1. Model dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f56a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "## MNIST dataset\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c73a7",
   "metadata": {},
   "source": [
    "### 2. Model configuration steps\n",
    "\n",
    "- 1. MNIST images are 28×28 pixels, thus we set both `image_width` & `image_height` to 28\n",
    "\n",
    "\n",
    "- 2. A batch size of 250 samples will be used\n",
    "\n",
    "\n",
    "- 3. The number of epochs will be 25. That is, data is fed to the NN 25 times (in batches of 250 samples)\n",
    "\n",
    "\n",
    "- 4. The number of classes will be 10 (ie, 0 to 9)\n",
    "\n",
    "\n",
    "- 5. 20% of the training data will be used for validaiton during optimisation\n",
    "\n",
    "\n",
    "- 6. To view as much output as possible, configure training to be `verbose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520dc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1.\n",
    "image_width, image_height = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d2906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2. \n",
    "batch_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c02270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3. \n",
    "number_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236d6873",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4. \n",
    "number_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4c439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5.\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262d48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6.\n",
    "verbosity = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320c3af",
   "metadata": {},
   "source": [
    "### 3. Loading & preparing MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da3dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load MNIST data\n",
    "(input_train, target_train), (input_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788fefa",
   "metadata": {},
   "source": [
    "### 4 Reshape Feature Vectors\n",
    "\n",
    "- 1. Reshape INPUT data (ie, Feature Vectors)\n",
    "\n",
    "\n",
    "- 2. Parse numbers as floats. This optimises the trade-off between memory & number precision \n",
    "\n",
    "\n",
    "- 3. Convert into interval (ie, 0, 1) range\n",
    "    - This converts the images into `greyscale` by dividing the image samples by 255\n",
    "    - Our only interest is in the actual number itself & not the colour of the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed54817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Old shape: (60000, 28, 28)\n",
      "> New shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"> Old shape: {input_train.shape}\")\n",
    "\n",
    "## Input Train\n",
    "input_train = input_train.reshape(input_train.shape[0], image_width, image_height, 1)\n",
    "print(f\"> New shape: {input_train.shape[0], image_width, image_height, 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386b0e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Old shape: (10000, 28, 28)\n",
      "> New shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "## Input Test\n",
    "print(f\"> Old shape: {input_test.shape}\")\n",
    "\n",
    "## Input Train\n",
    "input_test = input_test.reshape(input_test.shape[0], image_width, image_height, 1)\n",
    "print(f\"> New shape: {input_test.shape[0], image_width, image_height, 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121a9e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input Shape\n",
    "input_shape = (image_width, image_height, 1)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80eab2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2. \n",
    "input_train = input_train.astype('float32')\n",
    "input_test = input_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "756a4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3. \n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2c34b",
   "metadata": {},
   "source": [
    "## 5. Convert Target Vectors\n",
    "\n",
    "- Convert Target Vectors to Categorical Targets, ie from integers (0 - 9) into categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a38de0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Before conversion\n",
    "target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc9e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = tensorflow.keras.utils.to_categorical(target_train, number_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cb6e1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## After conversion\n",
    "target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7eb2c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Before conversion\n",
    "target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ab0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test = tensorflow.keras.utils.to_categorical(target_test, number_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbb4c66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## After conversion\n",
    "target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42b41a",
   "metadata": {},
   "source": [
    "## 6. Model Architecture\n",
    "#### Steps\n",
    "\n",
    "- 1. Create the model itself\n",
    "\n",
    "\n",
    "- 2. Begin with a 2D convolutional layer\n",
    "    - it learns 32 filters (or feature maps) based on the data\n",
    "    - the kernal, ie the small window that slides over the image is 3x3 pixels\n",
    "    - activation is 'relu' for nonlinearity\n",
    "    - input shape will be (28, 28, 1)\n",
    "\n",
    "\n",
    "- 3. This is followed with a MaxPooling2D layer with a pool size of 2x2\n",
    "    - this window slides over the 32 filters (see Step 2) \n",
    "    - for each slide it will take the maximum value & pass it on to the next layer\n",
    "    \n",
    "    \n",
    "- 4. Add Dropout to introduce random noise during training in an effort to reduce potential overfitting\n",
    "\n",
    "\n",
    "- 5. Repeat the 2D convolutional layer process\n",
    "    - this time with 64 filters (feature maps)\n",
    "    \n",
    "    \n",
    "- 6. Repeat the MaxPooling2D layer\n",
    "\n",
    "\n",
    "- 7. Repeat Dropout \n",
    "\n",
    "\n",
    "- 8. Convert filters learnt & processed into a flat structure (before predictions can be generated)\n",
    "\n",
    "\n",
    "- 9. Finally, allow data to be passed through two Dense layers\n",
    "    - the first which will be 'relu' activated\n",
    "    - the second will be 'softmax' activated to generate a multiclass probablilty distribution (ie, computes the probability that the 'item' belongs to one of the classes [0 > 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e41482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 15:17:36.326954: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## Step 1. \n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "692556f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2.\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5197702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3. \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94f19550",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4.\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46434e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5. \n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f5a6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6. \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31d0e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7. \n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83985388",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8. \n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cca98c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9. \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(number_class, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8dce15",
   "metadata": {},
   "source": [
    "### 7. Compile model\n",
    "\n",
    "- Configures the model architecture created above\n",
    "\n",
    "\n",
    "- Here the loss value, optimizer & additional metrics that are used during the `Training` process are defined\n",
    "\n",
    "\n",
    "- The loss functions is used to compute the difference between the actual targets & the targets **generated by the model** during an epoch\n",
    "\n",
    "\n",
    "- The higher the difference or the higher the loss, the worse the model will perform\n",
    "\n",
    "\n",
    "- The overall goal of the ML training process is to **MINIMISE LOSS**\n",
    "\n",
    "\n",
    "- Given that this is a Classification task, the function `cross entropy` will be used\n",
    "\n",
    "\n",
    "- This compares the actual outcomes with the **generated outcomes** computing the entropy / difficulty of successfully comparing the classes\n",
    "\n",
    "\n",
    "- Given that data is categorical - `categorical_crossentropy` is used\n",
    "\n",
    "\n",
    "- `Adaptive Moment Estimation` or `Adam` is selected for optimisation (the standard optimiser used today)\n",
    "\n",
    "\n",
    "- For the sake of being more intuitive to humans, accuracy is used as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cf1eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = tensorflow.keras.losses.categorical_crossentropy,\n",
    "              optimizer = tensorflow.keras.optimizers.Adam(),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156443e",
   "metadata": {},
   "source": [
    "### 8. Train model\n",
    "\n",
    "- ie, fit training data (both inputs & targets) to the model to initiate the training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd1c5005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 15:17:38.835267: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "192/192 [==============================] - 30s 156ms/step - loss: 0.3587 - accuracy: 0.8925 - val_loss: 0.1013 - val_accuracy: 0.9689\n",
      "Epoch 2/25\n",
      "192/192 [==============================] - 33s 170ms/step - loss: 0.0964 - accuracy: 0.9699 - val_loss: 0.0670 - val_accuracy: 0.9803\n",
      "Epoch 3/25\n",
      "192/192 [==============================] - 31s 159ms/step - loss: 0.0665 - accuracy: 0.9789 - val_loss: 0.0490 - val_accuracy: 0.9854\n",
      "Epoch 4/25\n",
      "192/192 [==============================] - 31s 163ms/step - loss: 0.0520 - accuracy: 0.9838 - val_loss: 0.0417 - val_accuracy: 0.9877\n",
      "Epoch 5/25\n",
      "192/192 [==============================] - 32s 169ms/step - loss: 0.0448 - accuracy: 0.9858 - val_loss: 0.0464 - val_accuracy: 0.9874\n",
      "Epoch 6/25\n",
      "192/192 [==============================] - 35s 182ms/step - loss: 0.0380 - accuracy: 0.9876 - val_loss: 0.0365 - val_accuracy: 0.9899\n",
      "Epoch 7/25\n",
      "192/192 [==============================] - 36s 187ms/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.0359 - val_accuracy: 0.9893\n",
      "Epoch 8/25\n",
      "192/192 [==============================] - 38s 195ms/step - loss: 0.0286 - accuracy: 0.9905 - val_loss: 0.0337 - val_accuracy: 0.9912\n",
      "Epoch 9/25\n",
      "192/192 [==============================] - 44s 230ms/step - loss: 0.0273 - accuracy: 0.9917 - val_loss: 0.0330 - val_accuracy: 0.9907\n",
      "Epoch 10/25\n",
      "192/192 [==============================] - 41s 211ms/step - loss: 0.0246 - accuracy: 0.9920 - val_loss: 0.0282 - val_accuracy: 0.9926\n",
      "Epoch 11/25\n",
      "192/192 [==============================] - 44s 227ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.0275 - val_accuracy: 0.9929\n",
      "Epoch 12/25\n",
      "192/192 [==============================] - 45s 235ms/step - loss: 0.0200 - accuracy: 0.9933 - val_loss: 0.0309 - val_accuracy: 0.9923\n",
      "Epoch 13/25\n",
      "192/192 [==============================] - 42s 217ms/step - loss: 0.0177 - accuracy: 0.9940 - val_loss: 0.0266 - val_accuracy: 0.9930\n",
      "Epoch 14/25\n",
      "192/192 [==============================] - 46s 241ms/step - loss: 0.0146 - accuracy: 0.9950 - val_loss: 0.0275 - val_accuracy: 0.9927\n",
      "Epoch 15/25\n",
      "192/192 [==============================] - 45s 235ms/step - loss: 0.0139 - accuracy: 0.9950 - val_loss: 0.0309 - val_accuracy: 0.9919\n",
      "Epoch 16/25\n",
      "192/192 [==============================] - 48s 249ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.0341 - val_accuracy: 0.9916\n",
      "Epoch 17/25\n",
      "192/192 [==============================] - 46s 239ms/step - loss: 0.0130 - accuracy: 0.9952 - val_loss: 0.0331 - val_accuracy: 0.9922\n",
      "Epoch 18/25\n",
      "192/192 [==============================] - 41s 214ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0308 - val_accuracy: 0.9920\n",
      "Epoch 19/25\n",
      "192/192 [==============================] - 45s 234ms/step - loss: 0.0101 - accuracy: 0.9966 - val_loss: 0.0270 - val_accuracy: 0.9929\n",
      "Epoch 20/25\n",
      "192/192 [==============================] - 41s 214ms/step - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.0331 - val_accuracy: 0.9914\n",
      "Epoch 21/25\n",
      "192/192 [==============================] - 45s 236ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0292 - val_accuracy: 0.9932\n",
      "Epoch 22/25\n",
      "192/192 [==============================] - 46s 241ms/step - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.0331 - val_accuracy: 0.9918\n",
      "Epoch 23/25\n",
      "192/192 [==============================] - 45s 232ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.0291 - val_accuracy: 0.9933\n",
      "Epoch 24/25\n",
      "192/192 [==============================] - 40s 208ms/step - loss: 0.0074 - accuracy: 0.9973 - val_loss: 0.0345 - val_accuracy: 0.9922\n",
      "Epoch 25/25\n",
      "192/192 [==============================] - 36s 185ms/step - loss: 0.0064 - accuracy: 0.9976 - val_loss: 0.0373 - val_accuracy: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a7a442fa0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_train, \n",
    "          target_train,\n",
    "          batch_size = batch_size,\n",
    "          epochs = number_epochs,\n",
    "          verbose = verbosity,\n",
    "          validation_split = validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1329ef",
   "metadata": {},
   "source": [
    "### 9. Adding test metrics for testing generalisation\n",
    "\n",
    "- Once the model is trained using both `Training` & `Validation` data, `Test` data can now be added to the model in order to evaluate the model’s predictive performance\n",
    "\n",
    "\n",
    "- This is executed after the Training proceess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5fae6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate\n",
    "score = model.evaluate(input_test, target_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "442f779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02900446392595768 / Test accuracy: 0.9926999807357788\n"
     ]
    }
   ],
   "source": [
    "## Generate \n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f149b",
   "metadata": {},
   "source": [
    "### 10. Interpreting model performance\n",
    "\n",
    "- In 25 epochs, the model has achieved a validation accuracy of approximately 99.76%, ie the model successfully predicticted the input to the network (CNN) 99% of the time. \n",
    "\n",
    "\n",
    "- The model also shows a similar performace for generalisation test executed (see 9) using the test data with an accuracy of 99.27%\n",
    "\n",
    "\n",
    "- Note that the Model loss is better than during training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
